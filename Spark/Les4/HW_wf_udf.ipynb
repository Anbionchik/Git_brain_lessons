{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "TLeTWw0yKfdA",
        "outputId": "49178d0d-11f2-4281-faf5-a3ddeb2fa42d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=fda99a8616d22360424db376f3bda324b47069abb05c6aeb4f75ee6eab305b00\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *"
      ],
      "metadata": {
        "id": "d6jiq3kyQhEB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3Q9g_UyNxS6"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "    .master(\"local[2]\")\\\n",
        "    .appName(\"Lesson_2\")\\\n",
        "    .config(\"spark.executor.instances\",2)\\\n",
        "    .config(\"spark.executor.memory\",'2g')\\\n",
        "    .config(\"spark.executor.cores\",1)\\\n",
        "    .getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.load('raw_sales.csv', format='csv', header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "TTrYZ5IpOuj6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for row in df.schema:\n",
        "  print(row)"
      ],
      "metadata": {
        "id": "nO3uoQClPIHU",
        "outputId": "94d55d94-25f3-454d-9d04-d0b3ab3df0e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StructField('datesold', TimestampType(), True)\n",
            "StructField('postcode', IntegerType(), True)\n",
            "StructField('price', IntegerType(), True)\n",
            "StructField('propertyType', StringType(), True)\n",
            "StructField('bedrooms', IntegerType(), True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVGNGR7pN1KC"
      },
      "source": [
        "# Самостоятельная работа к уроку 4\n",
        "На уроке мы попробовали оконные и пользовательские функции. Теперь закрепим полученные знания."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agigNChqOHnK"
      },
      "source": [
        "## Данные: [google drive: raw_sales.csv](https://drive.google.com/file/d/1G2N7Mnt4-Tqz4JdJxutGDMbJiOr32kZp/view?usp=sharing)\n",
        "\n",
        " Каждая строчка это продажа жилья, которая состоит из следующих полей (думаю описание не требуется):\n",
        "*   date of sale\n",
        "*   price\n",
        "*   property type\n",
        "*   number of bedrooms\n",
        "*   4digit postcode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xisyFowtQgx-"
      },
      "source": [
        "## Задание 1\n",
        "Добавьте к таблице следующие поля:\n",
        "*  Средняя стомость 10 проданных домов до текущего в том же районе (4digit postcode)\n",
        "*  Средняя стомость 10 проданных домов после текущего в том же районе (4digit postcode)\n",
        "*  Стоимость последнего проданного дома до текущего\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView('df')"
      ],
      "metadata": {
        "id": "RaPjqI9iQDQn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('''\n",
        "SELECT\n",
        "  postcode,\n",
        "  datesold,\n",
        "  AVG(price) OVER (ORDER BY postcode, datesold ROWS BETWEEN 10 PRECEDING AND 1 PRECEDING) AS AVG_PRICE_10_BEFORE,\n",
        "  AVG(price) OVER (ORDER BY postcode, datesold ROWS BETWEEN 1 FOLLOWING AND 10 FOLLOWING) AS AVG_PRICE_10_AFTER,\n",
        "  SUM(price) OVER (ORDER BY postcode, datesold ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING) AS PRICE_BEFORE\n",
        "FROM df\n",
        "ORDER BY postcode, datesold;''').show()"
      ],
      "metadata": {
        "id": "1XVIUjY0XgyX",
        "outputId": "91409e44-3d84-4708-e255-516314db961b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------------------+-------------------+------------------+------------+\n",
            "|postcode|           datesold|AVG_PRICE_10_BEFORE|AVG_PRICE_10_AFTER|PRICE_BEFORE|\n",
            "+--------+-------------------+-------------------+------------------+------------+\n",
            "|    2600|2007-07-08 00:00:00|               null|          708350.0|        null|\n",
            "|    2600|2007-08-16 00:00:00|           327000.0|          698350.0|      327000|\n",
            "|    2600|2007-12-05 00:00:00|           558500.0|          679350.0|      790000|\n",
            "|    2600|2008-01-21 00:00:00|  647333.3333333334|          742850.0|      825000|\n",
            "|    2600|2008-04-24 00:00:00|           564250.0|          786600.0|      315000|\n",
            "|    2600|2008-05-30 00:00:00|           509900.0|          839200.0|      292500|\n",
            "|    2600|2008-06-19 00:00:00|           479750.0|          868450.0|      329000|\n",
            "|    2600|2008-07-29 00:00:00|           520500.0|          805750.0|      765000|\n",
            "|    2600|2008-09-02 00:00:00|           571312.5|          715250.0|      927000|\n",
            "|    2600|2008-09-08 00:00:00|  661166.6666666666|          756250.0|     1380000|\n",
            "|    2600|2008-09-17 00:00:00|           669050.0|          741750.0|      740000|\n",
            "|    2600|2008-09-22 00:00:00|           708350.0|          730550.0|      720000|\n",
            "|    2600|2008-11-18 00:00:00|           698350.0|          755050.0|      690000|\n",
            "|    2600|2008-11-18 00:00:00|           679350.0|          701050.0|      635000|\n",
            "|    2600|2008-11-21 00:00:00|           742850.0|          729550.0|      950000|\n",
            "|    2600|2008-12-22 00:00:00|           786600.0|          716250.0|      730000|\n",
            "|    2600|2008-12-24 00:00:00|           839200.0|          641500.0|      855000|\n",
            "|    2600|2009-01-06 00:00:00|           868450.0|          672500.0|     1057500|\n",
            "|    2600|2009-01-12 00:00:00|           805750.0|          689000.0|      300000|\n",
            "|    2600|2009-01-20 00:00:00|           715250.0|          641500.0|      475000|\n",
            "+--------+-------------------+-------------------+------------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qvh2x6_8YU3F"
      },
      "source": [
        "## Задание 2\n",
        "В итоге у вас таблица с колонками (или нечто похожее):\n",
        "*   price\n",
        "*   Среднегодовая цена\n",
        "*  Средняя стомость 10 проданных домов до текущего в том же районе (4digit postcode) (1 балл)\n",
        "*  Средняя стомость 10 проданных домов после текущего в том же районе (4digit postcode) (1 балл)\n",
        "*  Стоимость последнего проданного дома до текущего ((1 балл)\n",
        "*  и др.\n",
        "\n",
        "Посчитайте кол-во уникальных значений в каждой строчке (unique(row)) (ипользуйте udf). Попробуйте сделать то же самое используя pandas udf."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prices = spark.sql('''\n",
        "SELECT\n",
        "  postcode,\n",
        "  datesold,\n",
        "  AVG(price) OVER (ORDER BY YEAR(datesold)) AS AVG_PRICE_YEAR,\n",
        "  AVG(price) OVER (ORDER BY postcode, datesold ROWS BETWEEN 10 PRECEDING AND 1 PRECEDING) AS AVG_PRICE_10_BEFORE,\n",
        "  AVG(price) OVER (ORDER BY postcode, datesold ROWS BETWEEN 1 FOLLOWING AND 10 FOLLOWING) AS AVG_PRICE_10_AFTER,\n",
        "  SUM(price) OVER (ORDER BY postcode, datesold ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING) AS PRICE_BEFORE\n",
        "FROM df\n",
        "ORDER BY postcode, datesold;''')"
      ],
      "metadata": {
        "id": "s0dt6k74UAWa"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "postcode = spark.sql('''SELECT postcode FROM df''')"
      ],
      "metadata": {
        "id": "bcIP9LftYkx5"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in prices.columns:\n",
        "  print(prices[col])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzXsadRzjdcQ",
        "outputId": "10f812e0-7290-4be4-fa71-9ac9e9fa3e88"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column<'postcode'>\n",
            "Column<'datesold'>\n",
            "Column<'AVG_PRICE_YEAR'>\n",
            "Column<'AVG_PRICE_10_BEFORE'>\n",
            "Column<'AVG_PRICE_10_AFTER'>\n",
            "Column<'PRICE_BEFORE'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "postcode.dropDuplicates().count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY7TQ6uaitZ6",
        "outputId": "13bd8cfb-8624-4efd-979c-ad5ee0b1009c"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@F.udf(returnType=IntegerType())\n",
        "def f(col):\n",
        "  return col.dropDuplicates().count()"
      ],
      "metadata": {
        "id": "Di2xFnunYJDh"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3EeEIMl-YhOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmSZTI9PAwQb"
      },
      "source": [
        "# Задание 3\n",
        "SQL like case when или if elif else\n",
        "\n",
        "Создайте колонку, в которой в которой будет отображаться \"+\", \"-\" или \"=\", если \"Средняя стомость 10 проданных домов до текущего в том же районе\" больше, меньше или равно \"Средняя стомость 10 проданных домов после текущего в том же районе (4digit postcode)\", соотвественно.\n",
        "\n",
        "Если одно из полей Null, запишите в эту колонку \"Нет данных\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3pfUThFQtE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbff8e90-2ecf-421f-8d8b-615fc538d777"
      },
      "source": [
        "spark.sql('''\n",
        "SELECT\n",
        "  postcode,\n",
        "  datesold,\n",
        "  AVG(price) OVER (ORDER BY postcode, datesold ROWS BETWEEN 10 PRECEDING AND 1 PRECEDING) AS AVG_PRICE_10_BEFORE,\n",
        "  AVG(price) OVER (ORDER BY postcode, datesold ROWS BETWEEN 1 FOLLOWING AND 10 FOLLOWING) AS AVG_PRICE_10_AFTER,\n",
        "  CASE\n",
        "  WHEN AVG_PRICE_10_BEFORE IS NULL OR \n",
        "      AVG_PRICE_10_AFTER IS NULL THEN 'Нет данных'\n",
        "  WHEN AVG_PRICE_10_BEFORE > AVG_PRICE_10_AFTER THEN \"+\"\n",
        "  WHEN AVG_PRICE_10_BEFORE < AVG_PRICE_10_AFTER THEN \"-\"\n",
        "  WHEN AVG_PRICE_10_BEFORE = AVG_PRICE_10_AFTER THEN \"=\"\n",
        "  END as price_comparison  \n",
        "FROM df\n",
        "ORDER BY postcode, datesold;''').show()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------------------+-------------------+------------------+----------------+\n",
            "|postcode|           datesold|AVG_PRICE_10_BEFORE|AVG_PRICE_10_AFTER|price_comparison|\n",
            "+--------+-------------------+-------------------+------------------+----------------+\n",
            "|    2600|2007-07-08 00:00:00|               null|          708350.0|      Нет данных|\n",
            "|    2600|2007-08-16 00:00:00|           327000.0|          698350.0|               -|\n",
            "|    2600|2007-12-05 00:00:00|           558500.0|          679350.0|               -|\n",
            "|    2600|2008-01-21 00:00:00|  647333.3333333334|          742850.0|               -|\n",
            "|    2600|2008-04-24 00:00:00|           564250.0|          786600.0|               -|\n",
            "|    2600|2008-05-30 00:00:00|           509900.0|          839200.0|               -|\n",
            "|    2600|2008-06-19 00:00:00|           479750.0|          868450.0|               -|\n",
            "|    2600|2008-07-29 00:00:00|           520500.0|          805750.0|               -|\n",
            "|    2600|2008-09-02 00:00:00|           571312.5|          715250.0|               -|\n",
            "|    2600|2008-09-08 00:00:00|  661166.6666666666|          756250.0|               -|\n",
            "|    2600|2008-09-17 00:00:00|           669050.0|          741750.0|               -|\n",
            "|    2600|2008-09-22 00:00:00|           708350.0|          730550.0|               -|\n",
            "|    2600|2008-11-18 00:00:00|           698350.0|          755050.0|               -|\n",
            "|    2600|2008-11-18 00:00:00|           679350.0|          701050.0|               -|\n",
            "|    2600|2008-11-21 00:00:00|           742850.0|          729550.0|               +|\n",
            "|    2600|2008-12-22 00:00:00|           786600.0|          716250.0|               +|\n",
            "|    2600|2008-12-24 00:00:00|           839200.0|          641500.0|               +|\n",
            "|    2600|2009-01-06 00:00:00|           868450.0|          672500.0|               +|\n",
            "|    2600|2009-01-12 00:00:00|           805750.0|          689000.0|               +|\n",
            "|    2600|2009-01-20 00:00:00|           715250.0|          641500.0|               +|\n",
            "+--------+-------------------+-------------------+------------------+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}