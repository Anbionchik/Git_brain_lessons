{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6gYDgPeN5WPC",
        "Yrxa6_7jjSpw",
        "CKlUurWBExd8",
        "Djo8QWzY_mUb",
        "Kt_wmIdcgOfB",
        "BMeCB4Vc6yqc"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2e2hbAH5HZh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from scipy.sparse import csr_matrix, coo_matrix\n",
        "\n",
        "from implicit.nearest_neighbours import ItemItemRecommender, CosineRecommender, TFIDFRecommender, BM25Recommender\n",
        "\n",
        "from implicit.evaluation import train_test_split\n",
        "from implicit.evaluation import precision_at_k, mean_average_precision_at_k, AUC_at_k, ndcg_at_k\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework 1**"
      ],
      "metadata": {
        "id": "6gYDgPeN5WPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1\n",
        "\n",
        "def recall_at_k(recommended_list, bought_list, k=5):\n",
        "    \n",
        "    bought_list = np.array(bought_list)\n",
        "    recommended_list = np.array(recommended_list)\n",
        "    \n",
        "    bought_list = bought_list  \n",
        "    recommended_list = recommended_list[:k]\n",
        "\n",
        "    flags = np.isin(bought_list, recommended_list)\n",
        "    \n",
        "    recall = flags.sum() / len(bought_list)\n",
        "    \n",
        "    return recall\n",
        "\n",
        "def money_recall_at_k(recommended_list, bought_list, prices_recommended, prices_bought, k=5):\n",
        "    bought_list = np.array(bought_list)\n",
        "    recommended_list = np.array(recommended_list)\n",
        "    prices_recommended = np.array(prices_recommended)\n",
        "    prices_bought = np.array(prices_bought)\n",
        "    \n",
        "    bought_list = bought_list\n",
        "    recommended_list = recommended_list[:k]\n",
        "    prices_recommended = prices_recommended[:k]\n",
        "    prices_bought = prices_bought[:k]\n",
        "\n",
        "    flags = np.isin(bought_list, recommended_list)\n",
        "    \n",
        "    recall = (flags*prices_bought).sum() / prices_bought.sum()\n",
        "    \n",
        "    return recall"
      ],
      "metadata": {
        "id": "8JkrQhl95bBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2\n",
        "def reciprocal_rank(recommended_list, bought_list):\n",
        "    ranks=0.\n",
        "    for item_rec in recommended_list:\n",
        "        for i, item_bought in enumerate(bought_list):\n",
        "            if item_rec == item_bought:\n",
        "                ranks += 1 / (i+1)\n",
        "    return ranks / len(recommended_list)"
      ],
      "metadata": {
        "id": "znzcEy7m-0S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework 2**"
      ],
      "metadata": {
        "id": "Yrxa6_7jjSpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1\n",
        "\n",
        "result = data_test.groupby('user_id')['item_id'].unique().reset_index()\n",
        "result.columns=['user_id', 'actual']\n",
        "\n",
        "result['actual'] = result['actual'].apply(lambda x: list(x))\n",
        "\n",
        "result.head(10)"
      ],
      "metadata": {
        "id": "xMdzW6WvjWHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_users = result.shape[0]\n",
        "new_test_users = len(set(data_test['user_id']) - set(data_train['user_id']))\n",
        "\n",
        "print('В тестовом дата сете {} юзеров'.format(test_users))\n",
        "print('В тестовом дата сете {} новых юзеров'.format(new_test_users))"
      ],
      "metadata": {
        "id": "EnzqOAXjlHg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_recommendation(items, n=5):\n",
        "\n",
        "    items = np.array(items)\n",
        "    recs = np.random.choice(items, size=n, replace=False)\n",
        "    \n",
        "    return recs.tolist()"
      ],
      "metadata": {
        "id": "HG_D1QzGlNCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "items = data_train.item_id.unique()\n",
        "print(items)\n",
        "\n",
        "result['rand rec'] = result['user_id'].apply(lambda x: random_recommendation(items, n=5))\n",
        "result.head(5)"
      ],
      "metadata": {
        "id": "qmnlZoMKlcrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hit_rate = 0\n",
        "for i in range(result.shape[0]):\n",
        "    flags = np.isin(result.loc[i,'actual'], result.loc[i,'rand rec'])\n",
        "    hit_rate += (flags.sum() > 0).astype(int)\n",
        "hit_rate"
      ],
      "metadata": {
        "id": "YWr9dA4Sldyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_random_recommendation(items, p, n=5):\n",
        "    \n",
        "    recs = np.random.choice(items, size=n, replace=False, p=p)\n",
        "    \n",
        "    return recs.tolist()"
      ],
      "metadata": {
        "id": "EcIi5buRlgMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_prep(df, func):\n",
        "    items = df.item_id.unique()\n",
        "\n",
        "    items = np.array(items)\n",
        "\n",
        "    count_df = df.groupby('item_id').item_id.agg(['count'])\n",
        "\n",
        "    count_df['weight'] = func(count_df['count'] / count_df['count'].sum())\n",
        "\n",
        "    items_df = pd.DataFrame(items, columns = ['item_id'])\n",
        "\n",
        "    probability_df = pd.merge(items_df, count_df, on='item_id', how='left')\n",
        "\n",
        "    probability_df.fillna(value=0, inplace=True)\n",
        "\n",
        "    p = np.array(probability_df.to_numpy()[:, 2])\n",
        "\n",
        "    p /= p.sum()  \n",
        "    \n",
        "    return items, p"
      ],
      "metadata": {
        "id": "gCDBDactljYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "items, p = data_prep(data_train, np.log)\n",
        "\n",
        "result['weighted log rand rec'] = \\\n",
        "        result['user_id'].apply(lambda x: weighted_random_recommendation(items, p, n=5))\n",
        "\n",
        "result.head(3)"
      ],
      "metadata": {
        "id": "O5MXDIASlqr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "items, p = data_prep(data_train, np.sqrt)\n",
        "\n",
        "result['weighted sqrt rand rec'] = \\\n",
        "            result['user_id'].apply(lambda x: weighted_random_recommendation(items, p, n=5))\n",
        "\n",
        "result.head(2)"
      ],
      "metadata": {
        "id": "ZH2GLzzZltNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "items, p = data_prep(data_train, np.square)\n",
        "\n",
        "result['weighted square rand rec'] = \\\n",
        "            result['user_id'].apply(lambda x: weighted_random_recommendation(items, p, n=5))\n",
        "\n",
        "result.head(2)"
      ],
      "metadata": {
        "id": "bw6nPbSZlvpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2\n",
        "\n",
        "def random_recommendation(items, n=5):\n",
        "\n",
        "    items = np.array(items)\n",
        "    recs = np.random.choice(items, size=n, replace=False)\n",
        "    \n",
        "    return recs.tolist()"
      ],
      "metadata": {
        "id": "ms1-LanomFEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "items = data_train.item_id.unique()\n",
        "\n",
        "result['rand rec'] = result['user_id'].apply(lambda x: random_recommendation(items, n=5))\n",
        "result.head(3)"
      ],
      "metadata": {
        "id": "-iEX-OCCmUwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def popularity_recommendation(data, n=5):\n",
        "    \n",
        "    popular = data.groupby('item_id')['sales_value'].sum().reset_index()\n",
        "    popular.sort_values('sales_value', ascending=False, inplace=True)\n",
        "    \n",
        "    recs = popular.head(n).item_id\n",
        "    \n",
        "    return recs.tolist()"
      ],
      "metadata": {
        "id": "JyJYuiXQmYuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "popular_recs = popularity_recommendation(data_train, n=5)\n",
        "\n",
        "result['popular rec'] = result['user_id'].apply(lambda x: popular_recs)\n",
        "result.head(3)"
      ],
      "metadata": {
        "id": "as2nI628mbgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "popularity = data_train.groupby('item_id')['quantity'].sum().reset_index()\n",
        "popularity.rename(columns={'quantity': 'n_sold'}, inplace=True)\n",
        "\n",
        "popularity.head()"
      ],
      "metadata": {
        "id": "6ID43Wssmdzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_5000 = popularity.sort_values('n_sold', ascending=False).head(5000).item_id.tolist()"
      ],
      "metadata": {
        "id": "eA7TXZt_tob0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.loc[~data_train['item_id'].isin(top_5000), 'item_id'] = 999999\n",
        "\n",
        "user_item_matrix = pd.pivot_table(data_train, \n",
        "                                  index='user_id', columns='item_id', \n",
        "                                  values='quantity',\n",
        "                                  aggfunc='count', \n",
        "                                  fill_value=0\n",
        "                                 )\n",
        "\n",
        "user_item_matrix"
      ],
      "metadata": {
        "id": "NjKuBTXNtqUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_item_matrix[user_item_matrix > 0] = 1  \n",
        "user_item_matrix = user_item_matrix.astype(float)\n",
        "\n",
        "sparse_user_item = csr_matrix(user_item_matrix).tocsr()\n",
        "\n",
        "user_item_matrix.head(3)"
      ],
      "metadata": {
        "id": "ycphsVvattpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users, items, interactions = data.user_id.nunique(), data.item_id.nunique(), data.shape[0]\n",
        "\n",
        "print('# users: ', users)\n",
        "print('# items: ', items)\n",
        "print('# interactions: ', interactions)"
      ],
      "metadata": {
        "id": "TdYHQYrqtzJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interactions / (users*items)"
      ],
      "metadata": {
        "id": "iZcMtOxxt0-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_item_matrix.sum().sum() / (user_item_matrix.shape[0] * user_item_matrix.shape[1]) * 100"
      ],
      "metadata": {
        "id": "hnfk_yFJt2xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sort(data.item_id.unique())"
      ],
      "metadata": {
        "id": "NXrjso3Ct5Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "userids = user_item_matrix.index.values\n",
        "itemids = user_item_matrix.columns.values\n",
        "\n",
        "matrix_userids = np.arange(len(userids))\n",
        "matrix_itemids = np.arange(len(itemids))\n",
        "\n",
        "id_to_itemid = dict(zip(matrix_itemids, itemids))\n",
        "id_to_userid = dict(zip(matrix_userids, userids))\n",
        "\n",
        "itemid_to_id = dict(zip(itemids, matrix_itemids))\n",
        "userid_to_id = dict(zip(userids, matrix_userids))"
      ],
      "metadata": {
        "id": "VN-Y5-sNt6qA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "model = ItemItemRecommender(K=5, num_threads=4) \n",
        "\n",
        "model.fit(csr_matrix(user_item_matrix).T.tocsr(),  \n",
        "          show_progress=True)\n",
        "\n",
        "recs = model.recommend(userid=userid_to_id[2], \n",
        "                        user_items=csr_matrix(user_item_matrix).tocsr(),  \n",
        "                        N=5, \n",
        "                        filter_already_liked_items=False, \n",
        "                        filter_items=None, "
      ],
      "metadata": {
        "id": "kiUxVXSot863"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "model = CosineRecommender(K=5, num_threads=4) \n",
        "\n",
        "model.fit(csr_matrix(user_item_matrix).T.tocsr(), \n",
        "          show_progress=True)\n",
        "\n",
        "recs = model.recommend(userid=userid_to_id[1], \n",
        "                        user_items=csr_matrix(user_item_matrix).tocsr(),   \n",
        "                        N=5, \n",
        "                        filter_already_liked_items=False, \n",
        "                        filter_items=None, \n",
        "                        recalculate_user=False)"
      ],
      "metadata": {
        "id": "BuL5qEV9uBkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "result['cosine'] = result['user_id'].\\\n",
        "    apply(lambda x: [id_to_itemid[rec[0]] for rec in \n",
        "                    model.recommend(userid=userid_to_id[x], \n",
        "                                    user_items=sparse_user_item,   \n",
        "                                    N=5, \n",
        "                                    filter_already_liked_items=False, \n",
        "                                    filter_items=None, \n",
        "                                    recalculate_user=True)])"
      ],
      "metadata": {
        "id": "BCw_rWjluIfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "model = TFIDFRecommender(K=5, num_threads=4) \n",
        "\n",
        "model.fit(csr_matrix(user_item_matrix).T.tocsr(), \n",
        "          show_progress=True)\n",
        "\n",
        "recs = model.recommend(userid=userid_to_id[1], \n",
        "                        user_items=csr_matrix(user_item_matrix).tocsr(),   \n",
        "                        N=5, \n",
        "                        filter_already_liked_items=False, \n",
        "                        filter_items=None, \n",
        "                        recalculate_user=False)"
      ],
      "metadata": {
        "id": "GV_t51UIuJGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "result['tfidf'] = result['user_id'].\\\n",
        "    apply(lambda x: [id_to_itemid[rec[0]] for rec in \n",
        "                    model.recommend(userid=userid_to_id[x], \n",
        "                                    user_items=sparse_user_item,   \n",
        "                                    N=5, \n",
        "                                    filter_already_liked_items=False, \n",
        "                                    filter_items=None, \n",
        "                                    recalculate_user=False)])"
      ],
      "metadata": {
        "id": "QXmgzd1YueCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in result.columns[2:]:\n",
        "    print(column, round(result.apply(lambda x: precision_at_k(x[column], x['actual'],  5), axis=1).mean(), 5))"
      ],
      "metadata": {
        "id": "6I3tslROukrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework 3**"
      ],
      "metadata": {
        "id": "CKlUurWBExd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "for factors in tqdm(np.linspace(30,170,8)):\n",
        "    for reg in np.geomspace(0.0001,0.01,8):\n",
        "\n",
        "        data_train.loc[~data_train['item_id'].isin(top_5000), 'item_id'] = 999999\n",
        "\n",
        "        user_item_matrix = pd.pivot_table(data_train, \n",
        "                                          index='user_id', columns='item_id', \n",
        "                                          values='quantity', \n",
        "                                          aggfunc='count', \n",
        "                                          fill_value=0\n",
        "                                         )\n",
        "\n",
        "        user_item_matrix = user_item_matrix.astype(float) \n",
        "\n",
        "        sparse_user_item = csr_matrix(user_item_matrix).tocsr()\n",
        "\n",
        "        user_item_matrix.head(3)\n",
        "\n",
        "        user_item_matrix = bm25_weight(user_item_matrix.T).T  \n",
        "\n",
        "        \n",
        "\n",
        "        model = AlternatingLeastSquares(factors=int(factors), \n",
        "                                        regularization=reg,\n",
        "                                        iterations=15, \n",
        "                                        calculate_training_loss=True, \n",
        "                                        num_threads=8) \n",
        "\n",
        "        model.fit(csr_matrix(user_item_matrix).T.tocsr(),  \n",
        "                  show_progress=False)\n",
        "\n",
        "        result[f'als_bm25_factors={factors}_reg={reg}'] = \\\n",
        "                        result['user_id'].apply(lambda x: get_recommendations(x, model=model, N=5))\n",
        "\n",
        "        result.apply(lambda row: precision_at_k(row[f'als_bm25_factors={factors}_reg={reg}'], \\\n",
        "                                                            row['actual']), axis=1).mean()"
      ],
      "metadata": {
        "id": "o41OhEulE2HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework 4**"
      ],
      "metadata": {
        "id": "Djo8QWzY_mUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Для работы с матрицами\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# Матричная факторизация\n",
        "from implicit.als import AlternatingLeastSquares\n",
        "from implicit.nearest_neighbours import ItemItemRecommender  # нужен для одного трюка\n",
        "from implicit.nearest_neighbours import bm25_weight, tfidf_weight\n",
        "\n",
        "\n",
        "class MainRecommender:\n",
        "    \"\"\"Рекоммендации, которые можно получить из ALS\n",
        "    \n",
        "    Input\n",
        "    -----\n",
        "    user_item_matrix: pd.DataFrame\n",
        "        Матрица взаимодействий user-item\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, data, weighting=True):\n",
        "        \n",
        "        # your_code. Это не обязательная часть. Но если вам удобно что-либо посчитать тут - можно это сделать\n",
        "        \n",
        "        self.user_item_matrix = self.prepare_matrix(data)  # pd.DataFrame\n",
        "        self.id_to_itemid, self.id_to_userid, \\ \n",
        "            self.itemid_to_id, self.userid_to_id = prepare_dicts(self.user_item_matrix)\n",
        "        \n",
        "        if weighting:\n",
        "            self.user_item_matrix = bm25_weight(self.user_item_matrix.T).T \n",
        "        \n",
        "        self.model = self.fit(self.user_item_matrix)\n",
        "        self.own_recommender = self.fit_own_recommender(self.user_item_matrix)\n",
        "     \n",
        "    \n",
        "    @staticmethod\n",
        "    def prepare_dicts(user_item_matrix):\n",
        "        \"\"\"Подготавливает вспомогательные словари\"\"\"\n",
        "        \n",
        "        userids = user_item_matrix.index.values\n",
        "        itemids = user_item_matrix.columns.values\n",
        "\n",
        "        matrix_userids = np.arange(len(userids))\n",
        "        matrix_itemids = np.arange(len(itemids))\n",
        "\n",
        "        id_to_itemid = dict(zip(matrix_itemids, itemids))\n",
        "        id_to_userid = dict(zip(matrix_userids, userids))\n",
        "\n",
        "        itemid_to_id = dict(zip(itemids, matrix_itemids))\n",
        "        userid_to_id = dict(zip(userids, matrix_userids))\n",
        "        \n",
        "        return id_to_itemid, id_to_userid, itemid_to_id, userid_to_id\n",
        "     \n",
        "    @staticmethod\n",
        "    def fit_own_recommender(user_item_matrix):\n",
        "        \"\"\"Обучает модель, которая рекомендует товары, среди товаров, купленных юзером\"\"\"\n",
        "    \n",
        "        own_recommender = ItemItemRecommender(K=1, num_threads=4)\n",
        "        own_recommender.fit(csr_matrix(user_item_matrix).T.tocsr())\n",
        "        \n",
        "        return own_recommender\n",
        "    \n",
        "    @staticmethod\n",
        "    def fit(user_item_matrix, n_factors=20, regularization=0.001, iterations=15, num_threads=4):\n",
        "        \"\"\"Обучает ALS\"\"\"\n",
        "        \n",
        "        model = AlternatingLeastSquares(factors=factors, \n",
        "                                             regularization=regularization,\n",
        "                                             iterations=iterations,  \n",
        "                                             num_threads=num_threads)\n",
        "        model.fit(csr_matrix(self.user_item_matrix).T.tocsr())\n",
        "        \n",
        "        return model\n",
        "\n",
        "def prefilter_items(data, item_features, take_n_popular=5000):\n",
        "    popularity = data.groupby('item_id')['user_id'].nunique().reset_index() \n",
        "    popularity['share_unique_users'] = popularity['user_id'] / data['user_id'].nunique()\n",
        "    \n",
        "    top_popular = popularity[popularity['share_unique_users'] > 0.5].item_id.tolist()\n",
        "    data = data[~data['item_id'].isin(top_popular)]\n",
        "    top_notpopular = popularity[popularity['share_unique_users'] < 0.01].item_id.tolist()\n",
        "    data = data[~data['item_id'].isin(top_notpopular)]\n",
        "\n",
        "    return data\n",
        "    "
      ],
      "metadata": {
        "id": "crE5W6Pz_pPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.metrics import precision_at_k, recall_at_k\n",
        "from src.utils import prefilter_items\n",
        "from src.recommenders import MainRecommender"
      ],
      "metadata": {
        "id": "eHwc3FH8EKGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework 5**"
      ],
      "metadata": {
        "id": "Kt_wmIdcgOfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Для работы с матрицами\n",
        "from scipy.sparse import csr_matrix, coo_matrix\n",
        "\n",
        "# Матричная факторизация\n",
        "from implicit.als import AlternatingLeastSquares\n",
        "from implicit.nearest_neighbours import bm25_weight, tfidf_weight\n",
        "\n",
        "from lightfm import LightFM\n",
        "from lightfm.evaluation import precision_at_k, recall_at_k\n",
        "\n",
        "# Функции из 1-ого вебинара\n",
        "import os, sys\n",
        "\n",
        "module_path = os.path.abspath(os.path.join(os.pardir))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "    \n",
        "# from src.metrics import precision_at_k, recall_at_k\n",
        "from src.utils import prefilter_items"
      ],
      "metadata": {
        "id": "8uN-REy4gShd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('../data/retail_train.csv')\n",
        "item_features = pd.read_csv('../raw_data/product.csv')\n",
        "user_features = pd.read_csv('../raw_data/hh_demographic.csv')\n",
        "\n",
        "# column processing\n",
        "item_features.columns = [col.lower() for col in item_features.columns]\n",
        "user_features.columns = [col.lower() for col in user_features.columns]\n",
        "\n",
        "item_features.rename(columns={'product_id': 'item_id'}, inplace=True)\n",
        "user_features.rename(columns={'household_key': 'user_id'}, inplace=True)\n",
        "\n",
        "# train test split\n",
        "test_size_weeks = 3\n",
        "\n",
        "data_train = data[data['week_no'] < data['week_no'].max() - test_size_weeks]\n",
        "data_test = data[data['week_no'] >= data['week_no'].max() - test_size_weeks]"
      ],
      "metadata": {
        "id": "w6DsJ2FyhvZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_items_before = data_train['item_id'].nunique()\n",
        "\n",
        "data_train = prefilter_items(data_train)\n",
        "\n",
        "n_items_after = data_train['item_id'].nunique()\n",
        "print('Decreased # items from {} to {}'.format(n_items_before, n_items_after))"
      ],
      "metadata": {
        "id": "31oHCjBthyil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_item_matrix = pd.pivot_table(data_train, \n",
        "                                  index='user_id', columns='item_id', \n",
        "                                  values='quantity', # Можно пробоват ьдругие варианты\n",
        "                                  aggfunc='count', \n",
        "                                  fill_value=0\n",
        "                                 )\n",
        "\n",
        "user_item_matrix = user_item_matrix.astype(float) # необходимый тип матрицы для implicit\n",
        "\n",
        "# переведем в формат saprse matrix\n",
        "sparse_user_item = csr_matrix(user_item_matrix).tocsr()"
      ],
      "metadata": {
        "id": "DUoDJbKsh2EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = data_test[data_test['item_id'].isin(data_train['item_id'].unique())]\n",
        "test_user_item_matrix = pd.pivot_table(data_test, \n",
        "                                  index='user_id', columns='item_id', \n",
        "                                  values='quantity', # Можно пробоват ьдругие варианты\n",
        "                                  aggfunc='count', \n",
        "                                  fill_value=0\n",
        "                                 )\n",
        "\n",
        "test_user_item_matrix = user_item_matrix.astype(float) # необходимый тип матрицы для i"
      ],
      "metadata": {
        "id": "RMWIsuA4h4HH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "userids = user_item_matrix.index.values\n",
        "itemids = user_item_matrix.columns.values\n",
        "\n",
        "matrix_userids = np.arange(len(userids))\n",
        "matrix_itemids = np.arange(len(itemids))\n",
        "\n",
        "id_to_itemid = dict(zip(matrix_itemids, itemids))\n",
        "id_to_userid = dict(zip(matrix_userids, userids))\n",
        "\n",
        "itemid_to_id = dict(zip(itemids, matrix_itemids))\n",
        "userid_to_id = dict(zip(userids, matrix_userids))"
      ],
      "metadata": {
        "id": "mru9oXZAh7Lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_feat = pd.DataFrame(user_item_matrix.index)\n",
        "user_feat = user_feat.merge(user_features, on='user_id', how='left')\n",
        "user_feat.set_index('user_id', inplace=True)\n",
        "\n",
        "item_feat = pd.DataFrame(user_item_matrix.columns)\n",
        "item_feat = item_feat.merge(item_features, on='item_id', how='left')\n",
        "item_feat.set_index('item_id', inplace=True)\n"
      ],
      "metadata": {
        "id": "47k_vhEyh9ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_feat_lightfm = pd.get_dummies(user_feat, columns=user_feat.columns.tolist())\n",
        "item_feat_lightfm = pd.get_dummies(item_feat, columns=item_feat.columns.tolist())"
      ],
      "metadata": {
        "id": "QxrWXtmXh_MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "?LightFM\n",
        "\n",
        "model = LightFM(no_components=30,  # Number of elements in embedding, K\n",
        "                loss='bpr', # 'warp'  # 'logistic'\n",
        "                learning_rate=0.07, \n",
        "                item_alpha=0.15, user_alpha=0.15, \n",
        "                random_state=42)\n",
        "\n",
        "# нужно отдельно подавать user-item матрицу и отдельно веса\n",
        "# если элемент матрицы больше 0, то заменяется на 1, если равен нулю, то остаётся нулю\n",
        "model.fit((sparse_user_item > 0) * 1,  # user-item matrix из 0 и 1\n",
        "          sample_weight=coo_matrix(user_item_matrix), # веса\n",
        "          \n",
        "          # эмбединги юзеров\n",
        "          user_features=csr_matrix(user_feat_lightfm.values).tocsr(),\n",
        "          \n",
        "          # эмбединги товаров\n",
        "          item_features=csr_matrix(item_feat_lightfm.values).tocsr(),\n",
        "          \n",
        "          # количество эпох - сколько раз обучающий датасет прогоняется по модели\n",
        "          epochs=15,\n",
        "          \n",
        "           # кол-во потоков (по кол-ву ядер процессора)\n",
        "          num_threads=4) "
      ],
      "metadata": {
        "id": "iGcn-JkMiBSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_emb = model.get_user_representations(features=csr_matrix(user_feat_lightfm.values).tocsr())\n"
      ],
      "metadata": {
        "id": "uRHhIBnuiNSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zbQkPRSeiN6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework 6**"
      ],
      "metadata": {
        "id": "BMeCB4Vc6yqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 1\n",
        "\n",
        "result_lvl_1 = data_val_lvl_1.groupby('user_id')['item_id'].unique().reset_index()\n",
        "result_lvl_1.columns=['user_id', 'actual']"
      ],
      "metadata": {
        "id": "lKoLewML64Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_train = data_train_lvl_1['user_id'].tolist()\n",
        "users_valid = result_lvl_1['user_id'].tolist()\n",
        "new_users = list(set(users_valid) - set(users_train))\n",
        "all_users = list(set(users_valid) & set(users_train))\n",
        "\n",
        "result_lvl_1 = result_lvl_1[~result_lvl_1['user_id'].isin(new_users)]"
      ],
      "metadata": {
        "id": "cA3s1INb7c2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_lvl_1['als'] = result_lvl_1['user_id'].apply(lambda x: recommender.get_als_recommendations(x, N=200))\n",
        "result_lvl_1.apply(lambda row: recall_at_k(row['als'], row['actual']), axis=1).mean()"
      ],
      "metadata": {
        "id": "kMjeioW_7eeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_lvl_1['own_recommendations'] = \\\n",
        "            result_lvl_1['user_id'].apply(lambda x: recommender.get_own_recommendations(x, N=200))\n",
        "result_lvl_1.apply(lambda row: recall_at_k(row['own_recommendations'], row['actual']), axis=1).mean()"
      ],
      "metadata": {
        "id": "wsCnl44L7gGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_lvl_1['similar_items_recommendation'] = \\\n",
        "            result_lvl_1['user_id'].apply(lambda x: recommender.get_similar_items_recommendation(x, N=200))\n",
        "result_lvl_1.apply(lambda row: recall_at_k(row['similar_items_recommendation'], row['actual']), axis=1).mean()"
      ],
      "metadata": {
        "id": "sxuAKBIM7jt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 2\n",
        "\n",
        "def new_user_features(data, user_features):\n",
        "    new_user_features = user_features.merge(data, on='user_id', how='left')\n",
        "    basket = new_user_features.groupby(['user_id'])['sales_value'].sum().reset_index()\n",
        "    baskets_qnt = new_user_features.groupby('user_id')['basket_id'].count().reset_index()\n",
        "    baskets_qnt.rename(columns={'basket_id': 'baskets_qnt'}, inplace=True)\n",
        "    average_basket = basket.merge(baskets_qnt)\n",
        "    average_basket['average_basket'] = average_basket.sales_value / average_basket.baskets_qnt\n",
        "    average_basket['sum_per_week'] = average_basket.sales_value / new_user_features.week_no.nunique()\n",
        "    average_basket = average_basket.drop(['sales_value', 'baskets_qnt'], axis=1)\n",
        "    user_features = user_features.merge(average_basket)\n",
        "\n",
        "    return user_features\n",
        "\n",
        "\n",
        "def new_item_features(data, item_features):\n",
        "    new_item_features = item_features.merge(data, on='item_id', how='left')\n",
        "    \n",
        "    price = new_item_features.groupby('item_id')['sales_value'].sum() \\\n",
        "                            / new_item_features.groupby('item_id')['quantity'].sum()\n",
        "    price = price.groupby('item_id').mean().reset_index()\n",
        "    price.columns = ['item_id', 'price']\n",
        "    price['price'].fillna(0, inplace= True)\n",
        "    \n",
        "\n",
        "    item_qnt = new_item_features.groupby(['item_id'])['quantity'].count().reset_index()\n",
        "    item_qnt.rename(columns={'quantity': 'quantity_of_sales'}, inplace=True)\n",
        "    item_qnt['quantity_of_sales_per_week'] = \\\n",
        "                item_qnt['quantity_of_sales'] / new_item_features['week_no'].nunique()\n",
        "    item_features = item_features.merge(item_qnt, on='item_id')\n",
        "    \n",
        "    return item_features\n",
        "\n",
        "def train_test_preprocessing(data):    \n",
        "    users_lvl_2 = pd.DataFrame(data['user_id'].unique())\n",
        "    users_lvl_2.columns = ['user_id']\n",
        "\n",
        "    train_users = data_train_lvl_1['user_id'].unique()\n",
        "    train_users.shape\n",
        "\n",
        "    users_lvl_2 = users_lvl_2[users_lvl_2['user_id'].isin(train_users)]\n",
        "    users_lvl_2_ = users_lvl_2.copy()\n",
        "    users_lvl_2['candidates'] = users_lvl_2['user_id'].apply(lambda x: recommender.get_own_recommendations(x, N=50))\n",
        "    \n",
        "    s = users_lvl_2.apply(lambda x: pd.Series(x['candidates']), axis=1).stack().reset_index(level=1, drop=True)\n",
        "    s.name = 'item_id'\n",
        "\n",
        "    users_lvl_2 = users_lvl_2.drop('candidates', axis=1).join(s)\n",
        "    \n",
        "    users_lvl_2['flag'] = 1\n",
        "\n",
        "    targets_lvl_2 = data[['user_id', 'item_id']].copy()\n",
        "    targets_lvl_2.head(2)\n",
        "\n",
        "    targets_lvl_2['target'] = 1  \n",
        "\n",
        "    targets_lvl_2 = users_lvl_2.merge(targets_lvl_2, on=['user_id', 'item_id'], how='left')\n",
        "\n",
        "    targets_lvl_2['target'].fillna(0, inplace= True)\n",
        "    targets_lvl_2.drop('flag', axis=1, inplace=True)\n",
        "\n",
        "    targets_lvl_2 = targets_lvl_2.merge(item_features, on='item_id', how='left')\n",
        "    targets_lvl_2 = targets_lvl_2.merge(user_features, on='user_id', how='left')\n",
        "\n",
        "    X = targets_lvl_2.drop('target', axis=1)\n",
        "    y = targets_lvl_2[['target']]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "\n",
        "def get_final_recomendation(X_test, test_preds_proba, data_val_lvl_2):\n",
        "    X_test['predict_proba'] = test_preds_proba\n",
        "\n",
        "    X_test.sort_values(['user_id', 'predict_proba'], ascending=False, inplace=True)\n",
        "\n",
        "    result = X_test.groupby('user_id').head(5)\n",
        "\n",
        "    recs = result.groupby('user_id')['item_id']\n",
        "    recomendations = []\n",
        "    for user, preds in recs:\n",
        "        recomendations.append({'user_id': user, 'recomendations': preds.tolist()})\n",
        "\n",
        "    recomendations = pd.DataFrame(recomendations)\n",
        "\n",
        "    result_lvl_2 = data_val_lvl_2.groupby('user_id')['item_id'].unique().reset_index()\n",
        "    result_lvl_2.columns=['user_id', 'actual']\n",
        "\n",
        "    result_lvl_2 = result_lvl_2.merge(recomendations)\n",
        "    \n",
        "    return result_lvl_2\n",
        "item_features = new_item_features(data_train_lvl_2, item_features)"
      ],
      "metadata": {
        "id": "TmUuL3Ha7lhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_features = new_user_features(data_train_lvl_2, user_features)"
      ],
      "metadata": {
        "id": "sBrYWi2K7r0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = train_test_preprocessing(data_train_lvl_2)\n",
        "cat_feats = X_train.columns[2:].tolist()\n",
        "X_train[cat_feats] = X_train[cat_feats].astype('category')\n",
        "X_test, y_test = train_test_preprocessing(data_val_lvl_2)\n",
        "X_test[cat_feats] = X_test[cat_feats].astype('category')\n",
        "lgb = LGBMClassifier(objective='binary', \n",
        "                     categorical_column=cat_feats)\n",
        "lgb.fit(X_train, y_train)\n",
        "\n",
        "test_preds_proba = lgb.predict_proba(X_test)[:, 1]"
      ],
      "metadata": {
        "id": "_YyIYKgb7uAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_lvl_2 = get_final_recomendation(X_test, test_preds_proba, data_val_lvl_2)"
      ],
      "metadata": {
        "id": "zrUVJrVk7zau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Course_project**"
      ],
      "metadata": {
        "id": "mFzbjeIRVOi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "# Для работы с матрицами\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# Матричная факторизация\n",
        "from implicit import als\n",
        "\n",
        "# Модель второго уровня\n",
        "from lightgbm import LGBMClassifier\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import os, sys\n",
        "module_path = os.path.abspath(os.path.join(os.pardir))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "\n",
        "# Написанные нами функции\n",
        "from src.metrics import precision_at_k, recall_at_k\n",
        "from src.utils import prefilter_items\n",
        "from src.recommenders import MainRecommender"
      ],
      "metadata": {
        "id": "_SQOQwBnVaR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('retail_train.csv')\n",
        "item_features = pd.read_csv('product.csv')\n",
        "user_features = pd.read_csv('hh_demographic.csv')"
      ],
      "metadata": {
        "id": "UVVLgFk6Wk8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_features.columns = [col.lower() for col in item_features.columns]\n",
        "user_features.columns = [col.lower() for col in user_features.columns]\n",
        "\n",
        "item_features.rename(columns={'product_id': 'item_id'}, inplace=True)\n",
        "user_features.rename(columns={'household_key': 'user_id'}, inplace=True)"
      ],
      "metadata": {
        "id": "gOuBZCIoWuuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_lvl_1_size_weeks = 6\n",
        "val_lvl_2_size_weeks = 3\n",
        "\n",
        "data_train_lvl_1 = data[data['week_no'] < data['week_no'].max() - (val_lvl_1_size_weeks + val_lvl_2_size_weeks)]\n",
        "data_val_lvl_1 = data[(data['week_no'] >= data['week_no'].max() - (val_lvl_1_size_weeks + val_lvl_2_size_weeks)) &\n",
        "                      (data['week_no'] < data['week_no'].max() - (val_lvl_2_size_weeks))]\n",
        "\n",
        "data_train_lvl_2 = data_val_lvl_1.copy()  # Для наглядности. Далее мы добавим изменения, и они будут отличаться\n",
        "data_val_lvl_2 = data[data['week_no'] >= data['week_no'].max() - val_lvl_2_size_weeks]"
      ],
      "metadata": {
        "id": "rpDNdG0dWu83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_lvl_1 = data_train_lvl_1.user_id.unique()\n",
        "users_lvl_2 = data_val_lvl_1.user_id.unique()\n",
        "users_lvl_3 = data_val_lvl_2.user_id.unique()\n",
        "\n",
        "new_users_lvl_2 = list(set(users_lvl_2) - set(users_lvl_1))\n",
        "new_users_lvl_3 = list(set(users_lvl_3) - (set(users_lvl_1) | set(users_lvl_2)))\n",
        "\n",
        "add_to_lvl_2 = list(set(users_lvl_3) - (set(users_lvl_2)))"
      ],
      "metadata": {
        "id": "-ZIeokmeWvPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_items_before = data_train_lvl_1['item_id'].nunique()\n",
        "data_train_lvl_1 = prefilter_items(data_train_lvl_1, item_features=item_features, take_n_popular=5000)\n",
        "n_items_after = data_train_lvl_1['item_id'].nunique()"
      ],
      "metadata": {
        "id": "yIW2P8nCWvsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_features['age_desc'].replace(\n",
        "    {'19-24': 22, '25-34': 30, '35-44': 40, '45-54': 50, '55-64': 60, '65+': 70},\n",
        "    inplace=True)\n",
        "\n",
        "user_features['marital_status_code'].replace(\n",
        "    {'U': 0, 'A': 1, 'B': 2}, inplace=True)\n",
        "\n",
        "user_features['income_desc'].replace(\n",
        "    {'Under 15K': 10, '15-24K': 20, '25-34K':30, '35-49K': 40,\n",
        "     '50-74K': 62, '75-99K': 87, '100-124K': 112, '125-149K': 137, \n",
        "     '150-174K': 162, '175-199K': 187, '200-249K': 225, '250K+':275}, inplace=True)\n",
        "\n",
        "user_features['homeowner_desc'].replace(\n",
        "    {'Unknown': 0, 'Probable Renter': 1, 'Renter': 2,\n",
        "     'Probable Owner': 3, 'Homeowner': 4}, inplace=True)\n",
        "\n",
        "user_features['hh_comp_desc'].replace(\n",
        "    {'Unknown': 0, 'Single Male': 1, 'Single Female': 2,\n",
        "     '1 Adult Kids': 3, '2 Adults No Kids': 4, '2 Adults Kids':5},inplace=True)\n",
        "\n",
        "user_features['household_size_desc'].replace({'5+': 5}, inplace=True) \n",
        "\n",
        "user_features['kid_category_desc'].replace(\n",
        "    {'None/Unknown': 0, '3+': 3}, inplace=True)"
      ],
      "metadata": {
        "id": "MwhG6KMVWv7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names = ['manufacturer', 'department', 'commodity_desc', 'sub_commodity_desc', 'curr_size_of_product']\n",
        "for name in names:\n",
        "    new_name = name + '_freq'\n",
        "    a = item_features[name].value_counts()\n",
        "    ind = a.index.tolist()\n",
        "    for i in ind:\n",
        "        item_features.loc[item_features[name] == i, new_name] = a[i]\n",
        "\n",
        "item_features['brand'] = np.where(item_features['brand']=='Private', 0, 1)\n",
        "\n",
        "commodities = item_features.commodity_desc.value_counts()\n",
        "commodities_list = commodities.keys().tolist()\n",
        "for i, name in enumerate(commodities_list):\n",
        "    item_features.loc[item_features['commodity_desc'] == name, 'commodity_category'] = i"
      ],
      "metadata": {
        "id": "2U-kPZDVWwJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_item_features(data_train_lvl_1):\n",
        "    X = data_train_lvl_1.copy()\n",
        "    X['hour'] = X['trans_time'] // 100\n",
        "    user_item_features = X.groupby(['user_id', 'item_id'])['hour'].median().reset_index()\n",
        "    user_item_features.columns = ['user_id', 'item_id', 'median_sales_hour']\n",
        "    \n",
        "    X['weekday'] = X['day'] % 7\n",
        "    df = X.groupby(['user_id', 'item_id'])['weekday'].median().reset_index()\n",
        "    df.columns = ['user_id', 'item_id', 'median_weekday']\n",
        "    user_item_features = user_item_features.merge(df, on=['user_id', 'item_id'])\n",
        "    \n",
        "    df = X.groupby('user_id')['day'].nunique().reset_index()\n",
        "    df['mean_visits_interval'] = (X.groupby('user_id')['day'].max() - X.groupby('user_id')['day'].min()) / df['day']\n",
        "    user_item_features = user_item_features.merge(df[['user_id', 'mean_visits_interval']], on=['user_id'])\n",
        "    \n",
        "    df = X.groupby(['user_id', 'basket_id'])['sales_value'].sum().reset_index()\n",
        "    df = df.groupby('user_id')['sales_value'].mean().reset_index()\n",
        "    df.columns = ['user_id', 'mean_check']\n",
        "    user_item_features = user_item_features.merge(df, on=['user_id'])\n",
        "    \n",
        "    df = X.groupby(['item_id'])['store_id'].nunique().reset_index()\n",
        "    df.columns = ['item_id', 'n_stores']\n",
        "    user_item_features = user_item_features.merge(df, on=['item_id'])\n",
        "    \n",
        "    df = X.groupby(['user_id'])['item_id'].nunique().reset_index()\n",
        "    df.columns = ['user_id', 'n_items']\n",
        "    user_item_features = user_item_features.merge(df, on=['user_id'])\n",
        "    \n",
        "    df = X.groupby(['user_id'])['item_id'].count().reset_index()\n",
        "    df.columns = ['user_id', 'n_transactions']\n",
        "    user_item_features = user_item_features.merge(df, on=['user_id'])\n",
        "    \n",
        "    df = X.groupby(['user_id', 'basket_id'])['item_id'].nunique().reset_index()\n",
        "    df1 = df.groupby('user_id')['item_id'].mean().reset_index()\n",
        "    df1.columns = ['user_id', 'mean_n_items_basket']\n",
        "    user_item_features = user_item_features.merge(df1, on=['user_id'])\n",
        "\n",
        "    df2 = df.groupby('user_id')['item_id'].max().reset_index()\n",
        "    df2.columns = ['user_id', 'max_n_items_basket']\n",
        "    user_item_features = user_item_features.merge(df2, on=['user_id'])\n",
        "\n",
        "    df3 = df.groupby('user_id')['item_id'].std().reset_index()\n",
        "    df3.columns = ['user_id', 'std_n_items_basket']\n",
        "    user_item_features = user_item_features.merge(df3, on=['user_id'])\n",
        "\n",
        "    recommender = MainRecommender(X)\n",
        "    df = recommender.model.item_factors\n",
        "    n_factors = recommender.model.factors\n",
        "    ind = list(recommender.id_to_itemid.values())\n",
        "    df = pd.DataFrame(df, index=ind).reset_index()\n",
        "    df.columns = ['item_id'] + ['factor_' + str(i + 1) for i in range(n_factors)]\n",
        "    user_item_features = user_item_features.merge(df, on=['item_id'])\n",
        "    \n",
        "    df = recommender.model.user_factors\n",
        "    ind = list(recommender.id_to_userid.values())\n",
        "    df = pd.DataFrame(df, index=ind).reset_index()\n",
        "    df.columns = ['user_id'] + ['user_factor_' + str(i + 1) for i in range(n_factors)]\n",
        "    user_item_features = user_item_features.merge(df, on=['user_id'])\n",
        "    \n",
        "    return user_item_features"
      ],
      "metadata": {
        "id": "pVLv1yfgXCD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_item_features = get_user_item_features(data_train_lvl_1)"
      ],
      "metadata": {
        "id": "U5NrbrNWXL0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_candidates(data_train_lvl_1, data_train_lvl_2, N, add_to_lvl_2):\n",
        "    recommender = MainRecommender(data_train_lvl_1)\n",
        "\n",
        "    users_lvl_1 = data_train_lvl_1['user_id'].unique()\n",
        "    users_lvl_2 = data_train_lvl_2['user_id'].unique().tolist()\n",
        "    if add_to_lvl_2:\n",
        "        users_lvl_2 += add_to_lvl_2\n",
        "\n",
        "    current_users = list(set(users_lvl_2) & set(users_lvl_1))    \n",
        "    new_users = list(set(users_lvl_2) - set(users_lvl_1))\n",
        "\n",
        "    df = pd.DataFrame(users_lvl_2, columns=['user_id'])\n",
        "    cond_1 = df['user_id'].isin(current_users)\n",
        "    df.loc[cond_1, 'candidates'] = df.loc[cond_1, 'user_id'].apply(\n",
        "        lambda x: recommender.get_own_recommendations(x, N))\n",
        "\n",
        "    if new_users:\n",
        "        cond_2 = df['user_id'].isin(new_users)\n",
        "        df.loc[cond_2, 'candidates'] = df.loc[cond_2, 'user_id'].apply(\n",
        "            lambda x: recommender.overall_top_purchases[:N])\n",
        "        \n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def get_targets_lvl_2(data_train_lvl_1, data_train_lvl_2, user_item_features, N, add_to_lvl_2=None):\n",
        "    \n",
        "    users_lvl_2 = get_candidates(data_train_lvl_1, data_train_lvl_2, N, add_to_lvl_2)\n",
        "    \n",
        "    df = pd.DataFrame({'user_id': users_lvl_2['user_id'].values.repeat(N),\n",
        "                       'item_id': np.concatenate(users_lvl_2['candidates'].values)})\n",
        "\n",
        "    targets_lvl_2 = data_train_lvl_2[['user_id', 'item_id']].copy()\n",
        "    targets_lvl_2['target'] = 1  \n",
        "\n",
        "    targets_lvl_2 = df.merge(targets_lvl_2, on=['user_id', 'item_id'], how='left')\n",
        "    targets_lvl_2['target'].fillna(0, inplace= True)\n",
        "    \n",
        "    targets_lvl_2 = targets_lvl_2.merge(\n",
        "        user_item_features, on=['user_id', 'item_id'], how='left')\n",
        "    \n",
        "    return targets_lvl_2\n",
        "N = 500\n",
        "targets_lvl_2 = get_targets_lvl_2(data_train_lvl_1, data_train_lvl_2, user_item_features, N, add_to_lvl_2)"
      ],
      "metadata": {
        "id": "aa5i84RXXODp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SELECTED_FEATURES_NAMES = ['brand', 'manufacturer_freq', 'department_freq', 'commodity_desc_freq',\n",
        "                           'sub_commodity_desc_freq', 'curr_size_of_product_freq',\n",
        "                           'commodity_category', 'age_desc', 'marital_status_code', 'income_desc',\n",
        "                           'homeowner_desc', 'hh_comp_desc'\n",
        "                           \n",
        "                             \n",
        "                           'manufacturer',\n",
        "                           \n",
        "                           'median_sales_hour', 'median_weekday', #'mean_visits_interval',\n",
        "                           'mean_check', \n",
        "                           'n_stores', 'n_items', 'n_transactions', \n",
        "                           'mean_n_items_basket', 'max_n_items_basket', 'std_n_items_basket',\n",
        "                           'mean_n_item_categories_basket', 'max_n_item_categories_basket', \n",
        "                           'std_n_item_categories_basket',\n",
        "                           'factor_1', 'factor_2', 'factor_3', 'factor_4', 'factor_5',\n",
        "                           'factor_6', 'factor_7', 'factor_8', 'factor_9', 'factor_10',\n",
        "                           'factor_11', 'factor_12', 'factor_13', 'factor_14', 'factor_15',\n",
        "                           'factor_16', 'factor_17', 'factor_18', 'factor_19', 'factor_20',\n",
        "                           \n",
        "                           'user_factor_1', 'user_factor_2', 'user_factor_3', 'user_factor_4',\n",
        "                           'user_factor_5', 'user_factor_6', 'user_factor_7', 'user_factor_8',\n",
        "                           'user_factor_9', 'user_factor_10', 'user_factor_11', 'user_factor_12',\n",
        "                           'user_factor_13', 'user_factor_14', 'user_factor_15', 'user_factor_16',\n",
        "                           'user_factor_17', 'user_factor_18', 'user_factor_19', 'user_factor_20',\n",
        "                          ]\n",
        "categorical = ['marital_status_code','homeowner_desc', 'hh_comp_desc', 'manufacturer','commodity_category']"
      ],
      "metadata": {
        "id": "ePeDrtOrXTTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SELECTED_FEATURES_NAMES_cb = [i for i in SELECTED_FEATURES_NAMES if not i in categorical]\n",
        "def run_model_cb(targets_lvl_2):    \n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(targets_lvl_2[SELECTED_FEATURES_NAMES_cb].fillna(0),\n",
        "                                                          targets_lvl_2[['target']],\n",
        "                                                          test_size=0.2, random_state=16,\n",
        "                                                          stratify=targets_lvl_2[['target']])\n",
        "\n",
        "    dtrain = Pool(data=X_train, label=y_train)\n",
        "    dvalid = Pool(data=X_valid, label=y_valid) \n",
        "\n",
        "    params_cb = {\"n_estimators\":5000,\n",
        "                 \"loss_function\": \"Logloss\",\n",
        "                 \"eval_metric\": \"AUC\",\n",
        "                 \"task_type\": \"CPU\",\n",
        "                 \"max_bin\": 30,\n",
        "                 \"early_stopping_rounds\": 30,\n",
        "                 \"verbose\": 1000,\n",
        "                 \"l2_leaf_reg\": 80,\n",
        "                 \"thread_count\": 6,\n",
        "                 \"random_seed\": 51} \n",
        "\n",
        "    model_cb = CatBoostClassifier(**params_cb)\n",
        "    model_cb.fit(dtrain, eval_set=[dvalid])\n",
        "\n",
        "    \n",
        "    return model_cb"
      ],
      "metadata": {
        "id": "D1UXK1juXWcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cb = run_model_cb(targets_lvl_2)\n",
        "\n",
        "print(model_cb.get_all_params())\n",
        "def run_model_lgb(targets_lvl_2):\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(targets_lvl_2[SELECTED_FEATURES_NAMES].fillna(0),\n",
        "                                                          targets_lvl_2[['target']],\n",
        "                                                          test_size=0.2, random_state=16,\n",
        "                                                          stratify=targets_lvl_2[['target']])\n",
        "\n",
        "    dtrain = lgb.Dataset(X_train, y_train, categorical_feature=categorical)\n",
        "    dvalid = lgb.Dataset(X_valid, y_valid, categorical_feature=categorical)\n",
        "\n",
        "    params_lgb = {\n",
        "                  \"objective\": \"binary\", \n",
        "                  \"metric\": \"auc\",\n",
        "                  \"num_boost_round\": 10000, \n",
        "                  \"n_jobs\": 8,\n",
        "                  \"force_row_wise\": True, \n",
        "                  \"seed\": 24} \n",
        "\n",
        "    model_lgb = lgb.train(params=params_lgb,\n",
        "                          train_set=dtrain,  \n",
        "                          valid_sets=[dtrain, dvalid],\n",
        "                          categorical_feature=categorical,\n",
        "                          verbose_eval=1000,\n",
        "                          early_stopping_rounds=30)\n",
        "    \n",
        "    return model_lgb\n",
        "model_lgb = run_model_lgb(targets_lvl_2)\n",
        "\n",
        "#[1322]\ttraining's auc: 0.954272\tvalid_1's auc: 0.912695"
      ],
      "metadata": {
        "id": "fbaYTix9XdAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_lgb_train = model_lgb.predict(targets_lvl_2[SELECTED_FEATURES_NAMES].fillna(0))\n",
        "predictions_cb_train = model_cb.predict_proba(targets_lvl_2[SELECTED_FEATURES_NAMES_cb].fillna(0))[:, 1]\n",
        "\n",
        "preds_train = pd.DataFrame(zip(predictions_lgb_train, \n",
        "                               predictions_cb_train),\n",
        "                           columns=['lgb', 'cb']).mean(axis=1).values\n",
        "roc_auc_score(targets_lvl_2['target'], preds_train)\n",
        "\n",
        "def get_predictions(targets_lvl_2, raw_predictions, prefix='lgb'): \n",
        "    df = targets_lvl_2[['user_id', 'item_id']]\n",
        "    df['predictions'] = raw_predictions\n",
        "\n",
        "    df = df.groupby(['user_id', 'item_id'])['predictions'].median().reset_index()\n",
        "    df = df.sort_values(['predictions'], ascending=False).groupby(['user_id']).head(5)\n",
        "\n",
        "    df = df.groupby('user_id')['item_id'].unique().reset_index()\n",
        "    df.columns = ['user_id', prefix + '_recommendations']\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def get_results(data_val_lvl_2, targets_lvl_2, preds_lgb, preds_cb, combined_preds):\n",
        "    result = data_val_lvl_2.groupby('user_id')['item_id'].unique().reset_index()\n",
        "    result.columns=['user_id', 'actual']\n",
        "    \n",
        "    prefixes = ['lgb', 'cb', 'cb_lgb']\n",
        "    predictions = [preds_lgb, preds_cb, combined_preds]\n",
        "    \n",
        "    for i, preds in enumerate(predictions):\n",
        "        df = get_predictions(targets_lvl_2, preds, prefixes[i])\n",
        "        result = result.merge(df, on='user_id', how='left')\n",
        "\n",
        "    return result\n",
        "result_lvl_2 = get_results(data_val_lvl_2, targets_lvl_2, \n",
        "                           predictions_lgb_train,\n",
        "                           predictions_cb_train, \n",
        "                           preds_train)"
      ],
      "metadata": {
        "id": "AAy4Wly9XnS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LightGBM\n",
        "result_lvl_2.apply(lambda row: precision_at_k(row['lgb_recommendations'], row['actual'], 5), axis=1).mean()\n",
        "# 0.29441723800195807\n",
        "\n",
        "#CatBoost\n",
        "result_lvl_2.apply(lambda row: precision_at_k(row['cb_recommendations'], row['actual'], 5), axis=1).mean()\n",
        "# 0.2863858961802145\n",
        "\n",
        "#Ensemble\n",
        "result_lvl_2.apply(lambda row: precision_at_k(row['cb_lgb_recommendations'], row['actual'], 5), axis=1).mean()\n",
        "# 0.30499510284035214"
      ],
      "metadata": {
        "id": "3vR4SDh0X6Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_weeks = 6\n",
        "data_train = data[data['week_no'] < data['week_no'].max() - validation_weeks]\n",
        "data_valid = data[data['week_no'] >= data['week_no'].max() - validation_weeks]\n",
        "test = pd.read_csv('predictions_basic.csv')\n",
        "users_lvl_1 = data_train.user_id.unique()\n",
        "users_lvl_2 = data_valid.user_id.unique()\n",
        "users_lvl_3 = test.user_id.unique()\n",
        "\n",
        "new_users_lvl_2 = list(set(users_lvl_2) - set(users_lvl_1))\n",
        "new_users_lvl_3 = list(set(users_lvl_3) - (set(users_lvl_1) | set(users_lvl_2)))\n",
        "\n",
        "add_to_lvl_2 = list(set(users_lvl_3) - (set(users_lvl_2)))\n",
        "\n",
        "new_users_lvl_2, new_users_lvl_3, len(add_to_lvl_2)\n",
        "\n",
        "from src.metrics import precision_at_k, recall_at_k\n",
        "from src.utils import prefilter_items\n",
        "from src.recommenders import MainRecommender\n",
        "import os\n",
        "\n",
        "os.chdir('src')\n",
        "%run utils.py\n",
        "n_items_before = data['item_id'].nunique()\n",
        "data_train = prefilter_items(data_train, item_features=item_features, take_n_popular=5000)\n",
        "n_items_after = data_train['item_id'].nunique()\n",
        "print('Decreased # items from {} to {}'.format(n_items_before, n_items_after))\n",
        "user_item_features = get_user_item_features(data_train)\n",
        "user_item_features.head(2)\n",
        "import os\n",
        "os.chdir('src')\n",
        "%run recommenders.py\n",
        "targets_test = get_targets_lvl_2(data_train, data_valid, user_item_features, N, add_to_lvl_2)\n",
        "\n",
        "print(f'число пользователей: {targets_test.user_id.nunique()}')\n",
        "print(f'среднее число покупок: {round(targets_test[\"target\"].mean(), 4)}')\n",
        "\n",
        "targets_test.head(2)\n",
        "SELECTED_FEATURES_NAMES = ['median_sales_hour', 'median_weekday', \n",
        "                           'mean_check', \n",
        "                           'n_stores', 'n_items', 'n_transactions', \n",
        "                           'mean_n_items_basket', 'max_n_items_basket', \n",
        "                           \n",
        "                          \n",
        "                           'factor_1', 'factor_2', 'factor_3', 'factor_4', 'factor_5',\n",
        "                           'factor_6', 'factor_7', 'factor_8', 'factor_9', 'factor_10',\n",
        "                           'factor_11', 'factor_12', 'factor_13', 'factor_14', 'factor_15',\n",
        "                           'factor_16', 'factor_17', 'factor_18', 'factor_19', 'factor_20',\n",
        "                           \n",
        "                           'user_factor_1', 'user_factor_2', 'user_factor_3', 'user_factor_4',\n",
        "                           'user_factor_5', 'user_factor_6', 'user_factor_7', 'user_factor_8',\n",
        "                           'user_factor_9', 'user_factor_10', 'user_factor_11', 'user_factor_12',\n",
        "                           'user_factor_13', 'user_factor_14', 'user_factor_15', 'user_factor_16',\n",
        "                           'user_factor_17', 'user_factor_18', 'user_factor_19', 'user_factor_20',\n",
        "                          ]\n",
        "categorical = []"
      ],
      "metadata": {
        "id": "aX1n4_z5YFdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lgb = run_model_lgb(targets_test)\n",
        "# [1558]\ttraining's auc: 0.900737\tvalid_1's auc: 0.87275\n",
        "SELECTED_FEATURES_NAMES_cb = [i for i in SELECTED_FEATURES_NAMES if not i in categorical]\n",
        "model_cb = run_model_cb(targets_test)\n",
        "# bestTest = 0.8620786343\n",
        "# bestIteration = 3899\n",
        "predictions_lgb_test = model_lgb.predict(targets_test[SELECTED_FEATURES_NAMES].fillna(0))\n",
        "predictions_cb_test = model_cb.predict_proba(targets_test[SELECTED_FEATURES_NAMES_cb].fillna(0))[:, 1]\n",
        "\n",
        "preds_test = pd.DataFrame(zip(predictions_lgb_test, predictions_cb_test),columns=['lgb', 'cb']).mean(axis=1).values\n",
        "roc_auc_score(targets_test['target'], preds_test)\n",
        "# 0.8887098749469134"
      ],
      "metadata": {
        "id": "KltiIQ6WYS6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_results_1(data_val_lvl_2, targets_lvl_2, preds_lgb, preds_cb, combined_preds):\n",
        "    result = data_val_lvl_2.groupby('user_id')['actual'].unique().reset_index()\n",
        "    result.columns=['user_id', 'actual']\n",
        "    \n",
        "    prefixes = ['lgb', 'cb', 'cb_lgb']\n",
        "    predictions = [preds_lgb, preds_cb, combined_preds]\n",
        "    \n",
        "    for i, preds in enumerate(predictions):\n",
        "        df = get_predictions(targets_lvl_2, preds, prefixes[i])\n",
        "        result = result.merge(df, on='user_id', how='left')\n",
        "\n",
        "    return result\n",
        "result_test = get_results_1(test, targets_test, \n",
        "                           predictions_lgb_test,\n",
        "                           predictions_cb_test, \n",
        "                           preds_test)"
      ],
      "metadata": {
        "id": "R1mtzaU6YZYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LightGBM\n",
        "result_test.apply(lambda row: precision_at_k(row['lgb_recommendations'], row['actual'], 5), axis=1).mean()\n",
        "# 0.3131034482758621\n",
        "\n",
        "#CatBooost\n",
        "result_test.apply(lambda row: precision_at_k(row['cb_recommendations'], row['actual'], 5), axis=1).mean()\n",
        "# 0.33389920424403186\n",
        "\n",
        "#Ensemble\n",
        "result_test.apply(lambda row: precision_at_k(row['cb_lgb_recommendations'], row['actual'], 5), axis=1).mean()\n",
        "# 0.3335809018567639\n",
        "Сохранение результатов\n",
        "\n",
        "df = result_test[['user_id', 'cb_lgb_recommendations']].copy()\n",
        "df.to_csv('predictions_2.csv', index=False)"
      ],
      "metadata": {
        "id": "UCtIt6IiYcJL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}